{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Running inference of T5 QG with BERT QE.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wrCIrtjpYnjT"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sacremoses"
      ],
      "metadata": {
        "id": "Wdd7Gi9cYz6X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentencepiece"
      ],
      "metadata": {
        "id": "nO9RQ3HnY-p9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/amontgomerie/question_generator/"
      ],
      "metadata": {
        "id": "eDTu1SGmZTb3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir(\"/content/\")\n",
        "!gdown '1KJ66LbUiPH-uLzuSMvJRTGjXrM-XnHCY'\n",
        "!gdown '1KHj25NWCVCZu7ViHz8bUAKL990ge_Jd4'\n",
        "file = \"/content/context.txt\"\n",
        "ref_file = \"/content/references.txt\"\n",
        "output_file_name = \"/content/bert_evaluated_hypothesis.txt\""
      ],
      "metadata": {
        "id": "OIj3SsUulNy1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "%cd question_generator/\n",
        "%load questiongenerator.py\n",
        "from questiongenerator import QuestionGenerator\n",
        "from questiongenerator import print_qa\n",
        "import torch\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "assert device == torch.device('cuda'), \"Not using CUDA. Set: Runtime > Change runtime type > Hardware Accelerator: GPU\"\n",
        "qg = QuestionGenerator()\n",
        "# file = 'articles/indian_matchmaking.txt'\n",
        "with open(file, 'r') as a:\n",
        "    article = a.read()\n",
        "qa_list = qg.generate(\n",
        "    article, \n",
        "    num_questions=10, \n",
        "    answer_style='all'\n",
        ")\n",
        "print_qa(qa_list)"
      ],
      "metadata": {
        "id": "_xldOq1DZbAo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qa_list = qg.generate(\n",
        "    article, \n",
        "    num_questions=81, \n",
        "    answer_style='all'\n",
        ")\n",
        "print_qa(qa_list)"
      ],
      "metadata": {
        "id": "_2uycwHvZtD4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for qa in qa_list:\n",
        "  print(qa)\n",
        "  break"
      ],
      "metadata": {
        "id": "4DYlQScYy8kB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "questions = [x['question']+'\\n' for x in qa_list]\n",
        "answers = [x['answer'] for x in qa_list]\n",
        "print(len(questions))"
      ],
      "metadata": {
        "id": "X-hj52Avz45i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out = open(output_file_name, \"w\")\n",
        "out.writelines(questions)\n",
        "out.close()"
      ],
      "metadata": {
        "id": "bAe6ZYn_0Hiq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "f3 = open(ref_file, \"r\")\n",
        "references = []\n",
        "blref = []\n",
        "for line in f3:\n",
        "#     print(line)\n",
        "    blref.insert(0, line)\n",
        "    references.insert(0, line.split())\n",
        "print(len(references))\n",
        "f3.close()"
      ],
      "metadata": {
        "id": "gZMcqNKP0RKY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we evaluate the generated questions using the BLEU and METEOR scores of the generated questions with respect to the reference questions. For this purpose we use the nltk library, the use of which is illustrated here."
      ],
      "metadata": {
        "id": "Z52WdZsQ0bM8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "hypothesis = ['It', 'is', 'a', 'cat', 'at', 'room']\n",
        "reference = ['It', 'is', 'a', 'cat', 'inside', 'the', 'room']\n",
        "#there may be several references\n",
        "BLEUscore = nltk.translate.bleu_score.sentence_bleu([reference], hypothesis)\n",
        "print(BLEUscore)\n",
        "METEOR = nltk.translate.meteor_score.meteor_score([reference], hypothesis)\n",
        "print(f\"{line}METEOR : {METEOR}\\n\")"
      ],
      "metadata": {
        "id": "1W-AAvRb0WWj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f2 = open(output_file_name, \"r\")\n",
        "i = 0\n",
        "BLEUdict = {}\n",
        "METEORdict = {}\n",
        "for line in f2:\n",
        "#     print(line)\n",
        "    i = i + 1\n",
        "    hypothesis = line\n",
        "    BLEUscore = nltk.translate.bleu_score.sentence_bleu(blref, hypothesis)\n",
        "    BLEUdict[line] = BLEUscore\n",
        "    print(f\"{i} : {line}BLEU4 : {BLEUscore}\\n\")\n",
        "    METEORscore = nltk.translate.meteor_score.meteor_score(references, hypothesis.split())\n",
        "    METEORdict[line] = METEORscore\n",
        "    print(f\"METEOR : {METEORscore}\\n\")\n",
        "f2.close()"
      ],
      "metadata": {
        "id": "25zvjogR0f-b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sorted(BLEUdict.items(), key=lambda item: item[-1], reverse = True)"
      ],
      "metadata": {
        "id": "ZFjzXXuG0jZL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sorted(METEORdict.items(), key=lambda item: item[1], reverse = True)"
      ],
      "metadata": {
        "id": "rPt9_NZq0kzU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sum(BLEUdict.values()) / len(BLEUdict.values()))\n",
        "print(sum(METEORdict.values()) / len(METEORdict.values()))\n",
        "print(sum(sorted(BLEUdict.values(),reverse = True)[0:12]) / 12)\n",
        "print(sum(sorted(METEORdict.values(),reverse = True)[0:12]) / 12)"
      ],
      "metadata": {
        "id": "_Y-VII9oWeF-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}